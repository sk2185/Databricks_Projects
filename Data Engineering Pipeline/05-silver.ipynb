{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "216d2f25-b427-4a17-bf3f-745059153282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad294b3-56e7-4cbc-9944-213703b61e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "landing_zone = base_dir_data + \"/raw\"\n",
    "checkpoint_base = base_dir_checkpoint + \"/checkpoints\"\n",
    "\n",
    "\n",
    "\n",
    "def upserter(df_micro_batch, batch_id, merge_query, temp_view):\n",
    "    df_micro_batch.createOrReplaceTempView(temp_view)\n",
    "    df_micro_batch._jdf.sparkSession().sql(merge_query)\n",
    "    print(f\"Batch {batch_id} for {temp_view} processed.\")\n",
    "\n",
    "\n",
    "\n",
    "def upsert_user_profile_microbatch(df_micro_batch, batch_id, merge_query):\n",
    "    # Define a window to rank records per user_id by updated timestamp descending\n",
    "    window = Window.partitionBy(\"user_id\").orderBy(F.col(\"updated\").desc())\n",
    "    \n",
    "    # Filter for only relevant update_types (\"new\", \"update\"), get latest per user_id\n",
    "    df_micro_batch = (\n",
    "        df_micro_batch.filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n",
    "        .withColumn(\"rank\", F.rank().over(window))\n",
    "        .filter(\"rank == 1\")\n",
    "        .drop(\"rank\")\n",
    "    )\n",
    "    \n",
    "    # Create or replace temp view for the MERGE SQL\n",
    "    df_micro_batch.createOrReplaceTempView(\"user_profile_cdc\")\n",
    "    \n",
    "    # Run the merge query to upsert into the Silver Delta table\n",
    "    df_micro_batch._jdf.sparkSession().sql(merge_query)\n",
    "    \n",
    "    print(f\"Batch {batch_id} processed.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upsert_users(once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "\n",
    "\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {catalog}.{db_name}.users a\n",
    "    USING users_delta b\n",
    "    ON a.user_id = b.user_id\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df_delta =(spark.readStream\n",
    "    .option(\"startingVersion\", startingVersion)\n",
    "    .option(\"ignoreDeletes\", \"true\")\n",
    "    .table(f\"{catalog}.{db_name}.registered_users_bz\")\n",
    "    .selectExpr(\"user_id\", \"device_id\", \"mac_address\", \"cast(registration_timestamp as timestamp)\")\n",
    "    .withWatermark(\"registration_timestamp\", \"30 seconds\")\n",
    "    .dropDuplicates([\"user_id\", \"device_id\"])\n",
    "    )\n",
    "\n",
    "\n",
    "    stream_writer =(df_delta.writeStream\n",
    "        .foreachBatch(lambda df, id: upserter(df, id, merge_query, \"users_delta\"))\n",
    "        .outputMode(\"update\")\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint_base}/users\")\n",
    "        .queryName(\"users_upsert_stream\")\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    if once == True:\n",
    "        return stream_writer.trigger(availableNow=True).start()\n",
    "    else:\n",
    "        return stream_writer.trigger(processingTime=processing_time).start()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upsert_gym_logs(once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {catalog}.{db_name}.gym_logs a\n",
    "    USING gym_logs_delta b\n",
    "    ON a.mac_address=b.mac_address AND a.gym=b.gym AND a.login=b.login\n",
    "    WHEN MATCHED AND b.logout > a.login AND b.logout > a.logout\n",
    "        THEN UPDATE SET logout = b.logout\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    df_delta = (spark.readStream\n",
    "                    .option(\"startingVersion\", startingVersion)\n",
    "                    .option(\"ignoreDeletes\", True)\n",
    "                    .table(f\"{catalog}.{db_name}.gym_logins_bz\")\n",
    "                    .selectExpr(\"mac_address\", \"gym\", \"cast(login as timestamp)\", \"cast(logout as timestamp)\")\n",
    "                    .withWatermark(\"login\", \"30 seconds\")\n",
    "                    .dropDuplicates([\"mac_address\", \"gym\", \"login\"])\n",
    "            )\n",
    "\n",
    "\n",
    "    stream_writer = (df_delta.writeStream\n",
    "                                .foreachBatch(lambda df, id: upserter(df, id, merge_query, \"gym_logs_delta\"))\n",
    "                                .outputMode(\"update\")\n",
    "                                .option(\"checkpointLocation\", checkpoint_base + \"/gyms_logs\")\n",
    "                                .queryName(\"gym_logs_upsert_stream\")\n",
    "                    )\n",
    "    \n",
    "\n",
    "\n",
    "    if once == True:\n",
    "        return stream_writer.trigger(availableNow=True).start()\n",
    "    else:\n",
    "        return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upsert_user_profile(once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "\n",
    "\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {catalog}.{db_name}.user_profile a\n",
    "    USING user_profile_cdc b\n",
    "    ON a.user_id=b.user_id\n",
    "    WHEN MATCHED AND a.updated < b.updated\n",
    "        THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED\n",
    "        THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    schema = \"\"\"\n",
    "    user_id bigint, update_type STRING, timestamp FLOAT, \n",
    "    dob STRING, sex STRING, gender STRING, first_name STRING, last_name STRING, \n",
    "    address STRUCT<street_address: STRING, city: STRING, state: STRING, zip: INT>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    df_cdc = (\n",
    "    spark.readStream\n",
    "            .option(\"startingVersion\", 0)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{catalog}.{db_name}.kafka_multiplex_bz\")\n",
    "            .filter(\"topic = 'user_info'\")\n",
    "            .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "            .select(\"v.*\")\n",
    "            .select(\n",
    "                \"user_id\",\n",
    "                F.to_date(\"dob\", \"MM/dd/yyyy\").alias(\"dob\"),\n",
    "                \"sex\", \"gender\", \"first_name\", \"last_name\",\n",
    "                \"address.*\",\n",
    "                F.col(\"timestamp\").cast(\"timestamp\").alias(\"updated\"),\n",
    "                \"update_type\"\n",
    "            )\n",
    "            .withWatermark(\"updated\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"updated\"])\n",
    "            )\n",
    "\n",
    "\n",
    "    stream_writer = (\n",
    "    df_cdc.writeStream\n",
    "            .foreachBatch(lambda df, id: upsert_user_profile_microbatch(df, id, merge_query))\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", checkpoint_base + \"/user_profile\")\n",
    "            .queryName(\"user_profile_stream\")\n",
    "\n",
    "             )\n",
    "    \n",
    "\n",
    "\n",
    "    if once == True:\n",
    "        return stream_writer.trigger(availableNow=True).start()\n",
    "    else:\n",
    "        return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upsert_workouts(once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {catalog}.{db_name}.workouts a\n",
    "    USING workouts_delta b\n",
    "    ON a.user_id=b.user_id AND a.time=b.time\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    schema = \"user_id INT, workout_id INT, timestamp FLOAT, action STRING, session_id INT\"\n",
    "\n",
    "    df_delta = (spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{catalog}.{db_name}.kafka_multiplex_bz\")\n",
    "            .filter(\"topic = 'workout'\")\n",
    "            .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "            .select(\"v.*\")\n",
    "            .select(\"user_id\", \"workout_id\", \n",
    "                    F.col(\"timestamp\").cast(\"timestamp\").alias(\"time\"), \n",
    "                    \"action\", \"session_id\")\n",
    "            .withWatermark(\"time\", \"30 seconds\")\n",
    "            .dropDuplicates([\"user_id\", \"time\"])\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    stream_writer = (df_delta.writeStream\n",
    "                                .foreachBatch(lambda df, id: upserter(df, id, merge_query, \"workouts_delta\"))\n",
    "                                .outputMode(\"update\")\n",
    "                                .option(\"checkpointLocation\",checkpoint_base + \"/workouts\")\n",
    "                                .queryName(\"workouts_upsert_stream\")\n",
    "                    )\n",
    "    \n",
    "\n",
    "    if once == True:\n",
    "        return stream_writer.trigger(availableNow=True).start()\n",
    "    else:\n",
    "        return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upsert_heart_rate(once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {catalog}.{db_name}.heart_rate a\n",
    "    USING heart_rate_delta b\n",
    "    ON a.device_id=b.device_id AND a.time=b.time\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    schema = \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\"\n",
    "\n",
    "\n",
    "    df_delta = (spark.readStream\n",
    "                        .option(\"startingVersion\", startingVersion)\n",
    "                        .option(\"ignoreDeletes\", True)\n",
    "                        .table(f\"{catalog}.{db_name}.kafka_multiplex_bz\")\n",
    "                        .filter(\"topic = 'bpm'\")\n",
    "                        .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                        .select(\"v.*\", F.when(F.col(\"v.heartrate\") <= 0, False).otherwise(True).alias(\"valid\"))\n",
    "                        .withWatermark(\"time\", \"30 seconds\")\n",
    "                        .dropDuplicates([\"device_id\", \"time\"])\n",
    "                )\n",
    "\n",
    "\n",
    "    stream_writer = (df_delta.writeStream\n",
    "                                .foreachBatch(lambda df, id: upserter(df, id, merge_query, \"heart_rate_delta\"))\n",
    "                                .outputMode(\"update\")\n",
    "                                .option(\"checkpointLocation\", checkpoint_base + \"/heart_rate\")\n",
    "                                .queryName(\"heart_rate_upsert_stream\")\n",
    "                    )\n",
    "\n",
    "\n",
    "    if once == True:\n",
    "        return stream_writer.trigger(availableNow=True).start()\n",
    "    else:\n",
    "        return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import floor, months_between, current_date, when, col\n",
    "\n",
    "def age_bins(dob_col):\n",
    "    age_col = floor(months_between(current_date(), dob_col) / 12)\n",
    "    \n",
    "    return (when(age_col < 18, \"under 18\")\n",
    "              .when((age_col >= 18) & (age_col < 25), \"18-25\")\n",
    "              .when((age_col >= 25) & (age_col < 35), \"25-35\")\n",
    "              .when((age_col >= 35) & (age_col < 45), \"35-45\")\n",
    "              .when((age_col >= 45) & (age_col < 55), \"45-55\")\n",
    "              .when((age_col >= 55) & (age_col < 65), \"55-65\")\n",
    "              .when((age_col >= 65) & (age_col < 75), \"65-75\")\n",
    "              .when((age_col >= 75) & (age_col < 85), \"75-85\")\n",
    "              .when((age_col >= 85) & (age_col < 95), \"85-95\")\n",
    "              .when(age_col >= 95, \"95+\")\n",
    "              .otherwise(\"invalid age\"))\n",
    "\n",
    "\n",
    "\n",
    "def upsert_user_bins(once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {catalog}.{db_name}.user_bins a\n",
    "    USING user_bins_delta b\n",
    "    ON a.user_id=b.user_id\n",
    "    WHEN MATCHED \n",
    "    THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Step 1: Load list of registered users from silver `users` table\n",
    "    df_user = spark.table(f\"{catalog}.{db_name}.users\").select(\"user_id\")\n",
    "\n",
    "\n",
    "    # Step 2: Read streaming changes from `user_profile` table\n",
    "    # - Set `ignoreChanges=True` to track only new or updated records (no deletes)\n",
    "\n",
    "    df_delta = (\n",
    "        spark.readStream\n",
    "            .option(\"startingVersion\", startingVersion)\n",
    "            .option(\"ignoreChanges\", True)\n",
    "            .table(f\"{catalog}.{db_name}.user_profile\")\n",
    "            .join(df_user, on=\"user_id\", how=\"left\")  # Statelss left join with static DataFrame\n",
    "            .select(\n",
    "                \"user_id\",\n",
    "                age_bins(col(\"dob\")).alias(\"age\"),  # Use previously defined function\n",
    "                \"gender\", \n",
    "                \"city\", \n",
    "                \"state\"\n",
    "            )\n",
    "    )\n",
    "\n",
    "\n",
    "    stream_writer = (df_delta.writeStream\n",
    "                            .foreachBatch(lambda df, id: upserter(df, id, merge_query, \"user_bins_delta\"))\n",
    "                            .outputMode(\"update\")\n",
    "                            .option(\"checkpointLocation\", checkpoint_base + \"/user_bins\")\n",
    "                            .queryName(\"user_bins_upsert_stream\")\n",
    "                    )\n",
    "\n",
    "\n",
    "    if once == True:\n",
    "        return stream_writer.trigger(availableNow=True).start()\n",
    "    else:\n",
    "        return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "\n",
    "def upsert_completed_workouts(once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "\n",
    "    #Idempotent - Only one user workout session completes. So ignore the duplicates and insert the new records\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {catalog}.{db_name}.completed_workouts a\n",
    "    USING completed_workouts_delta b\n",
    "    ON a.user_id=b.user_id AND a.workout_id = b.workout_id AND a.session_id=b.session_id\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df_start = (spark.readStream\n",
    "                        .option(\"startingVersion\", startingVersion)\n",
    "                        .option(\"ignoreDeletes\", True)\n",
    "                        .table(f\"{catalog}.{db_name}.workouts\")\n",
    "                        .filter(\"action = 'start'\")                         \n",
    "                        .selectExpr(\"user_id\", \"workout_id\", \"session_id\", \"time as start_time\")\n",
    "                        .withWatermark(\"start_time\", \"30 seconds\")\n",
    "                )\n",
    "\n",
    "    df_stop = (spark.readStream\n",
    "                        .option(\"startingVersion\", startingVersion)\n",
    "                        .option(\"ignoreDeletes\", True)\n",
    "                        .table(f\"{catalog}.{db_name}.workouts\")\n",
    "                        .filter(\"action = 'stop'\")                         \n",
    "                        .selectExpr(\"user_id\", \"workout_id\", \"session_id\", \"time as end_time\")\n",
    "                        .withWatermark(\"end_time\", \"30 seconds\")\n",
    "                )\n",
    "\n",
    "    # State cleanup - Define a condition to clean the state\n",
    "    #               - stop must occur within 3 hours of start \n",
    "    #               - stop < start + 3 hours\n",
    "    join_condition = [df_start.user_id == df_stop.user_id, df_start.workout_id==df_stop.workout_id, df_start.session_id==df_stop.session_id, \n",
    "                        df_stop.end_time < df_start.start_time + F.expr('interval 3 hour')]         \n",
    "\n",
    "    df_delta = (df_start.join(df_stop, join_condition)\n",
    "                        .select(df_start.user_id, df_start.workout_id, df_start.session_id, df_start.start_time, df_stop.end_time)\n",
    "                )\n",
    "\n",
    "    stream_writer = (df_delta.writeStream\n",
    "                                .foreachBatch(lambda df, id: upserter(df, id, merge_query, \"completed_workouts_delta\"))\n",
    "                                .outputMode(\"append\")\n",
    "                                .option(\"checkpointLocation\", checkpoint_base + \"/completed_workouts_delta\")\n",
    "                                .queryName(\"completed_workouts_upsert_stream\")\n",
    "                    )\n",
    "\n",
    "\n",
    "    if once == True:\n",
    "        return stream_writer.trigger(availableNow=True).start()\n",
    "    else:\n",
    "        return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "    \n",
    "\n",
    "def upsert_workout_bpm(once=True, processing_time=\"15 seconds\", startingVersion=0):\n",
    "\n",
    "    #Idempotent - Only one user workout session completes. So ignore the duplicates and insert the new records\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO {catalog}.{db_name}.workout_bpm a\n",
    "    USING workout_bpm_delta b\n",
    "    ON a.user_id=b.user_id AND a.workout_id = b.workout_id AND a.session_id=b.session_id AND a.time=b.time\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Load static user table\n",
    "    df_users = spark.read.table(f\"{catalog}.{db_name}.users\")\n",
    "\n",
    "    # Load the completed workouts stream\n",
    "    df_completed_workouts = (\n",
    "        spark.readStream\n",
    "            .option(\"startingVersion\", 0)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{catalog}.{db_name}.completed_workouts\")\n",
    "            .join(df_users, \"user_id\")\n",
    "            .selectExpr(\"user_id\", \"device_id\", \"workout_id\", \"session_id\", \"start_time\", \"end_time\")\n",
    "            .withWatermark(\"end_time\", \"30 seconds\")\n",
    "    )\n",
    "\n",
    "    # Load the heart rate stream\n",
    "    df_bpm = (\n",
    "        spark.readStream\n",
    "            .option(\"startingVersion\", 0)\n",
    "            .option(\"ignoreDeletes\", True)\n",
    "            .table(f\"{catalog}.{db_name}.heart_rate\")\n",
    "            .filter(\"valid = True\")\n",
    "            .selectExpr(\"device_id\", \"time\", \"heartrate\")\n",
    "            .withWatermark(\"time\", \"30 seconds\")\n",
    "    )\n",
    "\n",
    "    from pyspark.sql.functions import expr\n",
    "    # Define the join condition between BPM and workouts\n",
    "    join_condition = [\n",
    "        df_completed_workouts.device_id == df_bpm.device_id,\n",
    "        df_bpm.time > df_completed_workouts.start_time,\n",
    "        df_bpm.time <= df_completed_workouts.end_time,\n",
    "        df_completed_workouts.end_time < df_bpm.time + expr(\"interval 3 hours\")\n",
    "    ]\n",
    "\n",
    "    # Join and select the desired columns\n",
    "    df_delta = (\n",
    "        df_bpm.join(df_completed_workouts, join_condition)\n",
    "            .select(\"user_id\", \"workout_id\", \"session_id\", \"start_time\", \"end_time\", \"time\", \"heartrate\")\n",
    "    )\n",
    "\n",
    "    stream_writer = (\n",
    "        df_delta.writeStream\n",
    "                .foreachBatch(lambda df, id: upserter(df, id, merge_query, \"workout_bpm_delta\"))\n",
    "                .outputMode(\"append\")\n",
    "                .option(\"checkpointLocation\", checkpoint_base + \"/workout_bpm\")\n",
    "                .queryName(\"workout_bpm_upsert_stream\")\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    if once == True:\n",
    "        return stream_writer.trigger(availableNow=True).start()\n",
    "    else:\n",
    "        return stream_writer.trigger(processingTime=processing_time).start()\n",
    "\n",
    "\n",
    "\n",
    "def await_queries(once):\n",
    "    if once:\n",
    "        for stream in spark.streams.active:\n",
    "            stream.awaitTermination()\n",
    "\n",
    "\n",
    "def upsert_silver(once=True, processing_time=\"5 seconds\"):\n",
    "    import time\n",
    "    start = int(time.time())\n",
    "    print(f\"\\n Executing silver layer upsert ...\")\n",
    "\n",
    "    # Silver Layer 1: Core profile & device-level streams\n",
    "    upsert_users(once, processing_time)\n",
    "    upsert_gym_logs(once, processing_time)\n",
    "    upsert_user_profile(once, processing_time)\n",
    "    upsert_workouts(once, processing_time)\n",
    "    upsert_heart_rate(once, processing_time)\n",
    "\n",
    "    await_queries(once)\n",
    "    print(f\"Completed silver layer 1 upsert in {int(time.time()) - start} seconds\")\n",
    "\n",
    "    # Silver Layer 2: Derived user binning & session logs\n",
    "    upsert_user_bins(once, processing_time)\n",
    "    upsert_completed_workouts(once, processing_time)\n",
    "\n",
    "    await_queries(once)\n",
    "    print(f\" Completed silver layer 2 upsert in {int(time.time()) - start} seconds\")\n",
    "\n",
    "    # Silver Layer 3: Workout BPM aggregation\n",
    "    upsert_workout_bpm(once, processing_time)\n",
    "\n",
    "    await_queries(once)\n",
    "    print(f\" Completed silver layer 3 upsert in {int(time.time()) - start} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def assert_count(catalog, db_name, table_name, expected_count, filter=\"true\"):\n",
    "    print(f\"Validating record counts in {table_name}...\", end='')\n",
    "    actual_count = spark.read.table(f\"{catalog}.{db_name}.{table_name}\").where(filter).count()\n",
    "    assert actual_count == expected_count, f\"Expected {expected_count:,} records, found {actual_count:,} in {table_name} where {filter}\"\n",
    "    print(f\"Found {actual_count:,} / Expected {expected_count:,} records where {filter}: Success\")\n",
    "\n",
    "\n",
    "def validate_silver(catalog, db_name, sets):\n",
    "    import time\n",
    "    start = int(time.time())\n",
    "    print(f\"\\n Starting Silver layer validation...\")\n",
    "\n",
    "    # Silver layer 1\n",
    "    assert_count(catalog, db_name, \"users\", 5 if sets == 1 else 10)\n",
    "    assert_count(catalog, db_name, \"gym_logs\", 8 if sets == 1 else 16)\n",
    "    assert_count(catalog, db_name, \"user_profile\", 5 if sets == 1 else 10)\n",
    "    assert_count(catalog, db_name, \"workouts\", 16 if sets == 1 else 32)\n",
    "    assert_count(catalog, db_name, \"heart_rate\", sets * 253801)\n",
    "\n",
    "    print(f\"Silver layer 1 validation done in {int(time.time()) - start} seconds\\n\")\n",
    "\n",
    "    # Silver layer 2\n",
    "    assert_count(catalog, db_name, \"user_bins\", 5 if sets == 1 else 10)\n",
    "    assert_count(catalog, db_name, \"completed_workouts\", 8 if sets == 1 else 16)\n",
    "\n",
    "    print(f\"Silver layer 2 validation done in {int(time.time()) - start} seconds\\n\")\n",
    "\n",
    "    # Silver layer 3\n",
    "    assert_count(catalog, db_name, \"workout_bpm\", 3968 if sets == 1 else 8192)\n",
    "\n",
    "    print(f\"Silver layer 3 validation done in {int(time.time()) - start} seconds \")\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
